{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning problem : Classification \n",
    "\n",
    "multi-class-multi-label classification problem , because there are 9 broad categories that each take on many possible sub-label instances.\n",
    "\n",
    "websites : https://www.drivendata.org/competitions/46/box-plots-for-education-reboot/\n",
    "            https://www.erstrategies.org/\n",
    "            https://rawgit.com/wsurles/datacamp_courses/master/python_courses/machine_learning_with_the_experts_school_budgets/machine_learning_with_the_experts_school_budgets.html\n",
    "### Introduction to challenge :\n",
    "\n",
    "School budget in the US are incredibly complex, and there is no standards for importing how money is spent, school want to be able to measure their performance. Ex : are we spending more money on texbooks than the other schools? and is that investement worthwhile? doing this analysis takes hundreds of hours in which analysis hand-categorise each line-item of spending, our goal is to build a machine learning algorithm that can automate this process. \n",
    "\n",
    "Ex of line item expens/spendings : 'Algebra books for 8th grade Students'. \n",
    "this line has a set of labels attached to it which are our **target variables** : 'textbook', 'math', 'middleschoole' \n",
    "\n",
    "Our goal is to develop a model that predicts the probability for each possible label by relying on some correctly labeled examples.\n",
    "\n",
    "\n",
    "**Case Study/Challenge :**  Use data to have a social impact by solving a problem that is related to school district budgeting.  (Supervised Learning problem).\n",
    "\n",
    "**Goal:** By building a model to automatically classify items in a school's budget, it makes it easier and faster for schools to compare their spending with other schools.\n",
    "\n",
    "in details : we want to label the budget line items by training a supervised model to predict the probability of each possible label, taking most probable label as the correct label.\n",
    "\n",
    "\n",
    "**Approach** : building a baseline model that is a simple, first-pass approach. In particular, you'll do some natural language processing to prepare the budgets for modeling. Next, you'll have the opportunity to try your own techniques and see how they compare to participants from the competition. Finally, you'll see how the winner was able to combine a number of expert techniques to build the most accurate model.\n",
    "\n",
    "\n",
    "### Steps : \n",
    "\n",
    "* NLP : Natural Language Processing\n",
    "* Feature Engineering \n",
    "* Efficiency boosting hashing tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Exploring the raw data : \n",
    "\n",
    "**Take away** :   \n",
    "How do you accurately classify line-items in a school budget based on what that money is being used for? You will explore the raw text and numeric values in the dataset, both quantitatively and visually. And you'll learn how to measure success when trying to predict class labels for each row of the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Function</th>\n",
       "      <th>Use</th>\n",
       "      <th>Sharing</th>\n",
       "      <th>Reporting</th>\n",
       "      <th>Student_Type</th>\n",
       "      <th>Position_Type</th>\n",
       "      <th>Object_Type</th>\n",
       "      <th>Pre_K</th>\n",
       "      <th>Operating_Status</th>\n",
       "      <th>...</th>\n",
       "      <th>Sub_Object_Description</th>\n",
       "      <th>Location_Description</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Function_Description</th>\n",
       "      <th>Facility_or_Department</th>\n",
       "      <th>Position_Extra</th>\n",
       "      <th>Total</th>\n",
       "      <th>Program_Description</th>\n",
       "      <th>Fund_Description</th>\n",
       "      <th>Text_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>...</td>\n",
       "      <td>Non-Certificated Salaries And Wages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Care and Upkeep of Building Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8291.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n",
       "      <td>TITLE I CARRYOVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209</td>\n",
       "      <td>Student Transportation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Shared Services</td>\n",
       "      <td>Non-School</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Other Non-Compensation</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADMIN. SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STUDENT TRANSPORT SERVICE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618.29</td>\n",
       "      <td>PUPIL TRANSPORTATION</td>\n",
       "      <td>General Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750</td>\n",
       "      <td>Teacher Compensation</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>School Reported</td>\n",
       "      <td>School</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>Base Salary/Compensation</td>\n",
       "      <td>Non PreK</td>\n",
       "      <td>PreK-12 Operating</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEACHER</td>\n",
       "      <td>49768.82</td>\n",
       "      <td>Instruction - Regular</td>\n",
       "      <td>General Purpose School</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>931</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>...</td>\n",
       "      <td>General Supplies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Instruction</td>\n",
       "      <td>Instruction And Curriculum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>\"Title I, Part A Schoolwide Activities Related...</td>\n",
       "      <td>General Operating Fund</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1524</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>NO_LABEL</td>\n",
       "      <td>Non-Operating</td>\n",
       "      <td>...</td>\n",
       "      <td>Supplies And Materials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other Community Services *</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2304.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Title I - Disadvantaged Children/Targeted Assi...</td>\n",
       "      <td>TITLE I PI+HOMELESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                Function          Use          Sharing  \\\n",
       "0         198                NO_LABEL     NO_LABEL         NO_LABEL   \n",
       "1         209  Student Transportation     NO_LABEL  Shared Services   \n",
       "2         750    Teacher Compensation  Instruction  School Reported   \n",
       "3         931                NO_LABEL     NO_LABEL         NO_LABEL   \n",
       "4        1524                NO_LABEL     NO_LABEL         NO_LABEL   \n",
       "\n",
       "    Reporting Student_Type Position_Type               Object_Type     Pre_K  \\\n",
       "0    NO_LABEL     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "1  Non-School     NO_LABEL      NO_LABEL    Other Non-Compensation  NO_LABEL   \n",
       "2      School  Unspecified       Teacher  Base Salary/Compensation  Non PreK   \n",
       "3    NO_LABEL     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "4    NO_LABEL     NO_LABEL      NO_LABEL                  NO_LABEL  NO_LABEL   \n",
       "\n",
       "    Operating_Status          ...           \\\n",
       "0      Non-Operating          ...            \n",
       "1  PreK-12 Operating          ...            \n",
       "2  PreK-12 Operating          ...            \n",
       "3      Non-Operating          ...            \n",
       "4      Non-Operating          ...            \n",
       "\n",
       "                Sub_Object_Description Location_Description  FTE  \\\n",
       "0  Non-Certificated Salaries And Wages                  NaN  NaN   \n",
       "1                                  NaN      ADMIN. SERVICES  NaN   \n",
       "2                                  NaN                  NaN  1.0   \n",
       "3                     General Supplies                  NaN  NaN   \n",
       "4               Supplies And Materials                  NaN  NaN   \n",
       "\n",
       "                   Function_Description      Facility_or_Department  \\\n",
       "0  Care and Upkeep of Building Services                         NaN   \n",
       "1             STUDENT TRANSPORT SERVICE                         NaN   \n",
       "2                                   NaN                         NaN   \n",
       "3                           Instruction  Instruction And Curriculum   \n",
       "4            Other Community Services *                         NaN   \n",
       "\n",
       "  Position_Extra     Total                                Program_Description  \\\n",
       "0            NaN  -8291.86                                                NaN   \n",
       "1            NaN    618.29                               PUPIL TRANSPORTATION   \n",
       "2        TEACHER  49768.82                              Instruction - Regular   \n",
       "3            NaN     -1.02  \"Title I, Part A Schoolwide Activities Related...   \n",
       "4            NaN   2304.43                                                NaN   \n",
       "\n",
       "                                    Fund_Description                Text_1  \n",
       "0  Title I - Disadvantaged Children/Targeted Assi...     TITLE I CARRYOVER  \n",
       "1                                       General Fund                   NaN  \n",
       "2                             General Purpose School                   NaN  \n",
       "3                             General Operating Fund                   NaN  \n",
       "4  Title I - Disadvantaged Children/Targeted Assi...   TITLE I PI+HOMELESS  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1- Loading the Data\n",
    "df = pd.read_csv('TrainingData.csv')\n",
    "df.head()\n",
    "\n",
    "#YOU NEED TO SET INDEX TO df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1560 entries, 0 to 1559\n",
      "Data columns (total 26 columns):\n",
      "Unnamed: 0                1560 non-null int64\n",
      "Function                  1560 non-null object\n",
      "Use                       1560 non-null object\n",
      "Sharing                   1560 non-null object\n",
      "Reporting                 1560 non-null object\n",
      "Student_Type              1560 non-null object\n",
      "Position_Type             1560 non-null object\n",
      "Object_Type               1560 non-null object\n",
      "Pre_K                     1560 non-null object\n",
      "Operating_Status          1560 non-null object\n",
      "Object_Description        1461 non-null object\n",
      "Text_2                    382 non-null object\n",
      "SubFund_Description       1183 non-null object\n",
      "Job_Title_Description     1131 non-null object\n",
      "Text_3                    296 non-null object\n",
      "Text_4                    193 non-null object\n",
      "Sub_Object_Description    364 non-null object\n",
      "Location_Description      874 non-null object\n",
      "FTE                       449 non-null float64\n",
      "Function_Description      1340 non-null object\n",
      "Facility_or_Department    252 non-null object\n",
      "Position_Extra            1026 non-null object\n",
      "Total                     1542 non-null float64\n",
      "Program_Description       1192 non-null object\n",
      "Fund_Description          819 non-null object\n",
      "Text_1                    1132 non-null object\n",
      "dtypes: float64(2), int64(1), object(23)\n",
      "memory usage: 317.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FTE: Stands for \"full-time equivalent\". If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee.  \n",
    "- Total: Stands for the total cost of the expenditure. This number tells us how much the budget item cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FTE</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1560.000000</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>1.542000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>227767.180128</td>\n",
       "      <td>0.493532</td>\n",
       "      <td>1.446867e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>130207.535688</td>\n",
       "      <td>0.452844</td>\n",
       "      <td>7.916752e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>198.000000</td>\n",
       "      <td>-0.002369</td>\n",
       "      <td>-1.044084e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>113690.750000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>1.108111e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>226445.500000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>7.060299e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>340883.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.347760e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>450277.000000</td>\n",
       "      <td>1.047222</td>\n",
       "      <td>1.367500e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0         FTE         Total\n",
       "count    1560.000000  449.000000  1.542000e+03\n",
       "mean   227767.180128    0.493532  1.446867e+04\n",
       "std    130207.535688    0.452844  7.916752e+04\n",
       "min       198.000000   -0.002369 -1.044084e+06\n",
       "25%    113690.750000    0.004310  1.108111e+02\n",
       "50%    226445.500000    0.440000  7.060299e+02\n",
       "75%    340883.500000    1.000000  5.347760e+03\n",
       "max    450277.000000    1.047222  1.367500e+06"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2- summarizing the data for for the numeric data/columns \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The high variance in expenditures makes sense (some purchases are cheap some are expensive). Also, it looks like the FTE column is bimodal. That is, there are some part-time and some full-time employees.\n"
     ]
    }
   ],
   "source": [
    "# histogram of the non-null FTE column to see the distribution of part-time and full-time employees in the dataset.\n",
    "\n",
    "plt.hist(df['FTE'].dropna(), edgecolor = 'k')\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employees')\n",
    "plt.show()\n",
    "print('The high variance in expenditures makes sense (some purchases are cheap some are expensive). Also, it looks like the FTE column is bimodal. That is, there are some part-time and some full-time employees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     23\n",
      "float64     2\n",
      "int64       1\n",
      "dtype: int64\n",
      "There is a lot of string data(23) that we need to convert to numerical type since ML works on numbers\n"
     ]
    }
   ],
   "source": [
    "# 3- Looking at the datatypes \n",
    "\n",
    "print(df.dtypes.value_counts())\n",
    "print('There is a lot of string data(23) that we need to convert to numerical type since ML works on numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 columns of labels in the dataset. Each of these columns is a category that has many possible values it can take).\n"
     ]
    }
   ],
   "source": [
    "#4- Encode the labels as categorical variables\n",
    "\n",
    "print('There are 9 columns of labels in the dataset. Each of these columns is a category that has many possible values it can take).')\n",
    "\n",
    "labels = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type','Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function            object\n",
       "Use                 object\n",
       "Sharing             object\n",
       "Reporting           object\n",
       "Student_Type        object\n",
       "Position_Type       object\n",
       "Object_Type         object\n",
       "Pre_K               object\n",
       "Operating_Status    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[labels].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Use                 category\n",
      "Sharing             category\n",
      "Reporting           category\n",
      "Student_Type        category\n",
      "Position_Type       category\n",
      "Object_Type         category\n",
      "Pre_K               category\n",
      "Operating_Status    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[labels] to a categorical type\n",
    "df[labels] = df[labels].apply(categorize_label, axis = 0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[labels].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            37\n",
      "Use                  8\n",
      "Sharing              5\n",
      "Reporting            3\n",
      "Student_Type         9\n",
      "Position_Type       25\n",
      "Object_Type         11\n",
      "Pre_K                3\n",
      "Operating_Status     3\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " That's a lot of labels to work with. How will you measure success with these many labels? \n"
     ]
    }
   ],
   "source": [
    "#5 - Counting unique labels: (there are over 100 unique labels)\n",
    "num_unique_labels = df[labels].apply(pd.Series.nunique)\n",
    "print(num_unique_labels)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind = 'bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(\" That's a lot of labels to work with. How will you measure success with these many labels? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we measure success?  \n",
    "\n",
    "choosing how to evaluate machine learning model: log loss/ loss function instead of accuracy\n",
    "\n",
    "**accuracy** : is a simple measure that gives the % the rows we got right. accuracy can be misleading when classes are not imbalanced. \n",
    "\n",
    "**log loss** : is a measure of error, we want our error to be small as possible(unlike accuracy where we want to maximize the value).\n",
    "\n",
    "###### Log loss binary classification : \n",
    "\n",
    "it takes the actual value(yi) 1 or 0, and it takes our prediction(pi) which is a probability btw 0 and 1\n",
    "\n",
    "logloss=âˆ’1Nâˆ‘i=1N(yi log(pi)+(1âˆ’yi)log(1âˆ’pi))\n",
    "\n",
    "\n",
    "\n",
    "Log loss binary classification: example\n",
    "logloss(N=1)=ylog(p)+(1âˆ’y)log(1âˆ’p)\n",
    " \n",
    "*case 1 : True label (yi) = 0\n",
    "Model confidently predicts 1 (with p = 0.90 the model is confident) \n",
    "Log loss =\n",
    "ylog(p) + (1 - y)log(1 - p)\n",
    "0log(.9) + (1 - 0)log(1 - .9)\n",
    "0 + log(0.1)\n",
    "**2.30** the error is large means the model is wrong dispite p = 0.90\n",
    "\n",
    "*case2 : label (yi) = 1\n",
    "Model predicts 0 (with p = 0.50 )\n",
    "log loss = **0.69** the error is minimal comparing to first case which means model is accurate.\n",
    "\n",
    "P.S : better to be less confident than confident and wrong. \n",
    "\n",
    "Penalizing highly confident wrong answers : log loss provides a steep penalty for predictions that are both wrong and confident, i.e., a high probability is assigned to the incorrect class.\n",
    "\n",
    "N.B \n",
    "- np.clip function : sets a maximum and a minimum value for the elements of an array.\n",
    "- eps : since log(0) is a negative infinity, we want to offset our predictions ever so slightly from being exacly 1 or exaclty 0 so that our score remain a real number so we use eps=1e-14 (close enough to 0 not to affect our overall score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302585092994046\n",
      "0.6931471805599453\n",
      "Log loss penalizes highly confident wrong answers much more than any other type. This will be a good metric to use on your models\n"
     ]
    }
   ],
   "source": [
    "## Computing log loss\n",
    "\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    \"\"\"Computes the logarithmic los between predicted and actual when these are 1D arrays.\n",
    "    \n",
    "    :param predicted: The predicted probabilities as floats bwtween 0-1\n",
    "    :param actual: The actual binary labels. Either 0 or 1.\n",
    "    :param eps (optional): log(0) is inf, so we need to offset our predicted values slightly by eps from 0 or 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# runing on previous case 1 example \n",
    "print(compute_log_loss(0.90, 0))\n",
    "\n",
    "# runing on previous case 2 example \n",
    "print(compute_log_loss(0.50, 1))\n",
    "\n",
    "print('Log loss penalizes highly confident wrong answers much more than any other type. This will be a good metric to use on your models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Creating a simple first model : based only on numerical columns\n",
    "\n",
    "it's always good approach to start with a very simple model, it helps give a sens of how much challenging the problem is. We will start with a model that uses just the numeric data columns, in this first model we want to go from raw data to predictions as quickly as possible. In this case we will use multi-class logistic regression, which treat each label column as idependant. the model will train logistic regression classifier for each of the numerical columns separatly and then use those models to predict whether the label appears or not for any given rows. \n",
    "\n",
    "The approach of split_train_test wont work in this case because of nature of data, because some labels don't occur very often, but we want to make sure that they appear in both the training and the test sets, so the solution is : **StratifiedSuffleSplit** (this sklean function only works with a signle target variable, in our case we have many target variables). To work around this issue we have provided a utility function **multilabel_train_test_split()** that will ensure that all of the classes are represented in both train and test sets. \n",
    "\n",
    "\n",
    "Steps to build the model :\n",
    "- Step 1: Subset the data to only numerical columns. and preprocess by filling the NaN with -1000(we want our algorithm to respond to NaN's diffrently than 0).\n",
    "- Step 2: creat an array of target variables using get_dummies(takes target variables and produces binary indicators). \n",
    "- Step 3: Split_train_test using multilabel_train_test_split(). \n",
    "_ Step 4: importing LogisticRegression and oneVsRest classifer(let us treat each column of y independently, it fits a separate clissifier for each column), (it's a strategy used when we have multiple classes, there are other strategies to use).\n",
    "- Step 5: train the classifier on x_tain, y_train data. \n",
    "- Step 6: predictiong on X_test data and evaluating the model with logloss. \n",
    "\n",
    "*link to **multilabel_train_test_split()** (his function will ensue that atleast min_count examples of each label appear in each split): https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/data/multilabel.py\n",
    "\n",
    "P.S : Remember, we're ultimately going to be using logloss to score our model, so don't worry too much about the accuracy here. Keep in mind that you're throwing away all of the text data in the dataset - that's by far most of the data! So don't get your hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel_train_test_split() function:\n",
    "\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).all():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FTE</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1000.0</td>\n",
       "      <td>-8291.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1000.0</td>\n",
       "      <td>618.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>49768.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1000.0</td>\n",
       "      <td>-1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1000.0</td>\n",
       "      <td>2304.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FTE     Total\n",
       "0 -1000.0  -8291.86\n",
       "1 -1000.0    618.29\n",
       "2     1.0  49768.82\n",
       "3 -1000.0     -1.02\n",
       "4 -1000.0   2304.43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STEP 1: Create the new DataFrame: numeric_data_only\n",
    "numeric_columns = ['FTE', 'Total']\n",
    "\n",
    "numeric_data_only = df[numeric_columns].fillna(-1000)\n",
    "\n",
    "numeric_data_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function_Aides Compensation</th>\n",
       "      <th>Function_Career &amp; Academic Counseling</th>\n",
       "      <th>Function_Communications</th>\n",
       "      <th>Function_Curriculum Development</th>\n",
       "      <th>Function_Data Processing &amp; Information Services</th>\n",
       "      <th>Function_Development &amp; Fundraising</th>\n",
       "      <th>Function_Enrichment</th>\n",
       "      <th>Function_Extended Time &amp; Tutoring</th>\n",
       "      <th>Function_Facilities &amp; Maintenance</th>\n",
       "      <th>Function_Facilities Planning</th>\n",
       "      <th>...</th>\n",
       "      <th>Object_Type_Rent/Utilities</th>\n",
       "      <th>Object_Type_Substitute Compensation</th>\n",
       "      <th>Object_Type_Supplies/Materials</th>\n",
       "      <th>Object_Type_Travel &amp; Conferences</th>\n",
       "      <th>Pre_K_NO_LABEL</th>\n",
       "      <th>Pre_K_Non PreK</th>\n",
       "      <th>Pre_K_PreK</th>\n",
       "      <th>Operating_Status_Non-Operating</th>\n",
       "      <th>Operating_Status_Operating, Not PreK-12</th>\n",
       "      <th>Operating_Status_PreK-12 Operating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Function_Aides Compensation  Function_Career & Academic Counseling  \\\n",
       "0                            0                                      0   \n",
       "1                            0                                      0   \n",
       "2                            0                                      0   \n",
       "3                            0                                      0   \n",
       "4                            0                                      0   \n",
       "\n",
       "   Function_Communications  Function_Curriculum Development  \\\n",
       "0                        0                                0   \n",
       "1                        0                                0   \n",
       "2                        0                                0   \n",
       "3                        0                                0   \n",
       "4                        0                                0   \n",
       "\n",
       "   Function_Data Processing & Information Services  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   Function_Development & Fundraising  Function_Enrichment  \\\n",
       "0                                   0                    0   \n",
       "1                                   0                    0   \n",
       "2                                   0                    0   \n",
       "3                                   0                    0   \n",
       "4                                   0                    0   \n",
       "\n",
       "   Function_Extended Time & Tutoring  Function_Facilities & Maintenance  \\\n",
       "0                                  0                                  0   \n",
       "1                                  0                                  0   \n",
       "2                                  0                                  0   \n",
       "3                                  0                                  0   \n",
       "4                                  0                                  0   \n",
       "\n",
       "   Function_Facilities Planning                 ...                  \\\n",
       "0                             0                 ...                   \n",
       "1                             0                 ...                   \n",
       "2                             0                 ...                   \n",
       "3                             0                 ...                   \n",
       "4                             0                 ...                   \n",
       "\n",
       "   Object_Type_Rent/Utilities  Object_Type_Substitute Compensation  \\\n",
       "0                           0                                    0   \n",
       "1                           0                                    0   \n",
       "2                           0                                    0   \n",
       "3                           0                                    0   \n",
       "4                           0                                    0   \n",
       "\n",
       "   Object_Type_Supplies/Materials  Object_Type_Travel & Conferences  \\\n",
       "0                               0                                 0   \n",
       "1                               0                                 0   \n",
       "2                               0                                 0   \n",
       "3                               0                                 0   \n",
       "4                               0                                 0   \n",
       "\n",
       "   Pre_K_NO_LABEL  Pre_K_Non PreK  Pre_K_PreK  Operating_Status_Non-Operating  \\\n",
       "0               1               0           0                               1   \n",
       "1               1               0           0                               0   \n",
       "2               0               1           0                               0   \n",
       "3               1               0           0                               1   \n",
       "4               1               0           0                               1   \n",
       "\n",
       "   Operating_Status_Operating, Not PreK-12  Operating_Status_PreK-12 Operating  \n",
       "0                                        0                                   0  \n",
       "1                                        0                                   1  \n",
       "2                                        0                                   1  \n",
       "3                                        0                                   0  \n",
       "4                                        0                                   0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[labels])\n",
    "\n",
    "label_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040 entries, 0 to 1559\n",
      "Data columns (total 2 columns):\n",
      "FTE      1040 non-null float64\n",
      "Total    1040 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 24.4 KB\n",
      "None\n",
      "\n",
      "X_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 520 entries, 1 to 1549\n",
      "Data columns (total 2 columns):\n",
      "FTE      520 non-null float64\n",
      "Total    520 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 12.2 KB\n",
      "None\n",
      "\n",
      "y_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040 entries, 0 to 1559\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 113.8 KB\n",
      "None\n",
      "\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 520 entries, 1 to 1549\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 56.9 KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n"
     ]
    }
   ],
   "source": [
    "# step 3: Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only, label_dummies,size=0.2, seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "The good news is that your workflow didn't cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0! But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss.\n"
     ]
    }
   ],
   "source": [
    "# Step 4 and step 5 \n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Print the accuracy for the results of training.\n",
    "print(\"Accuracy: {}\".format(clf.score( X_test, y_test)))\n",
    "\n",
    "print(\"The good news is that your workflow didn't cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0! But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if .predict() was used instead the output would be 0 or 1, log loss penalizes being confident and wrong, we would have gotten worse performance compared to .predict_proba()\n"
     ]
    }
   ],
   "source": [
    "# Step 6: predictiong on X_test data and evaluating the model with logloss \n",
    "\n",
    "\n",
    "# extacting numerical columns and filling NaN's with -1000\n",
    "X_test_data = X_test[numeric_columns].fillna(-1000)\n",
    "\n",
    "# predictions for X_test\n",
    "predictions = clf.predict_proba(X_test_data)\n",
    "\n",
    "#building dataframe for the probability predictions\n",
    "col_names = pd.get_dummies(df[labels]).columns\n",
    "df_test_predictions = pd.DataFrame(data =predictions, columns =col_names, index = y_test.index)\n",
    "\n",
    "df_test_predictions.head()\n",
    "\n",
    "print(\"if .predict() was used instead the output would be 0 or 1, log loss penalizes being confident and wrong, we would have gotten worse performance compared to .predict_proba()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function_Aides Compensation</th>\n",
       "      <th>Function_Career &amp; Academic Counseling</th>\n",
       "      <th>Function_Communications</th>\n",
       "      <th>Function_Curriculum Development</th>\n",
       "      <th>Function_Data Processing &amp; Information Services</th>\n",
       "      <th>Function_Development &amp; Fundraising</th>\n",
       "      <th>Function_Enrichment</th>\n",
       "      <th>Function_Extended Time &amp; Tutoring</th>\n",
       "      <th>Function_Facilities &amp; Maintenance</th>\n",
       "      <th>Function_Facilities Planning</th>\n",
       "      <th>...</th>\n",
       "      <th>Object_Type_Rent/Utilities</th>\n",
       "      <th>Object_Type_Substitute Compensation</th>\n",
       "      <th>Object_Type_Supplies/Materials</th>\n",
       "      <th>Object_Type_Travel &amp; Conferences</th>\n",
       "      <th>Pre_K_NO_LABEL</th>\n",
       "      <th>Pre_K_Non PreK</th>\n",
       "      <th>Pre_K_PreK</th>\n",
       "      <th>Operating_Status_Non-Operating</th>\n",
       "      <th>Operating_Status_Operating, Not PreK-12</th>\n",
       "      <th>Operating_Status_PreK-12 Operating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Function_Aides Compensation  Function_Career & Academic Counseling  \\\n",
       "1                             0                                      0   \n",
       "3                             0                                      0   \n",
       "9                             0                                      0   \n",
       "11                            0                                      0   \n",
       "12                            0                                      0   \n",
       "\n",
       "    Function_Communications  Function_Curriculum Development  \\\n",
       "1                         0                                0   \n",
       "3                         0                                0   \n",
       "9                         0                                0   \n",
       "11                        0                                0   \n",
       "12                        0                                0   \n",
       "\n",
       "    Function_Data Processing & Information Services  \\\n",
       "1                                                 0   \n",
       "3                                                 0   \n",
       "9                                                 0   \n",
       "11                                                0   \n",
       "12                                                0   \n",
       "\n",
       "    Function_Development & Fundraising  Function_Enrichment  \\\n",
       "1                                    0                    0   \n",
       "3                                    0                    0   \n",
       "9                                    0                    0   \n",
       "11                                   0                    0   \n",
       "12                                   0                    0   \n",
       "\n",
       "    Function_Extended Time & Tutoring  Function_Facilities & Maintenance  \\\n",
       "1                                   0                                  0   \n",
       "3                                   0                                  0   \n",
       "9                                   0                                  0   \n",
       "11                                  0                                  0   \n",
       "12                                  0                                  0   \n",
       "\n",
       "    Function_Facilities Planning                 ...                  \\\n",
       "1                              0                 ...                   \n",
       "3                              0                 ...                   \n",
       "9                              0                 ...                   \n",
       "11                             0                 ...                   \n",
       "12                             0                 ...                   \n",
       "\n",
       "    Object_Type_Rent/Utilities  Object_Type_Substitute Compensation  \\\n",
       "1                            0                                    0   \n",
       "3                            0                                    0   \n",
       "9                            0                                    0   \n",
       "11                           0                                    0   \n",
       "12                           0                                    0   \n",
       "\n",
       "    Object_Type_Supplies/Materials  Object_Type_Travel & Conferences  \\\n",
       "1                                0                                 0   \n",
       "3                                0                                 0   \n",
       "9                                0                                 0   \n",
       "11                               0                                 0   \n",
       "12                               0                                 0   \n",
       "\n",
       "    Pre_K_NO_LABEL  Pre_K_Non PreK  Pre_K_PreK  \\\n",
       "1                1               0           0   \n",
       "3                1               0           0   \n",
       "9                1               0           0   \n",
       "11               1               0           0   \n",
       "12               1               0           0   \n",
       "\n",
       "    Operating_Status_Non-Operating  Operating_Status_Operating, Not PreK-12  \\\n",
       "1                                0                                        0   \n",
       "3                                1                                        0   \n",
       "9                                0                                        0   \n",
       "11                               0                                        0   \n",
       "12                               0                                        0   \n",
       "\n",
       "    Operating_Status_PreK-12 Operating  \n",
       "1                                    1  \n",
       "3                                    0  \n",
       "9                                    1  \n",
       "11                                   1  \n",
       "12                                   1  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test (Actual results)\n",
    "\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21690826180176895\n"
     ]
    }
   ],
   "source": [
    "# make sure that format is correct so we can compare btw prediction and actual results\n",
    "assert (df_test_predictions.columns == y_test.columns).all()\n",
    "assert (df_test_predictions.index == y_test.index).all()\n",
    "\n",
    "# Evaluating the model with log loss \n",
    "print(compute_log_loss(df_test_predictions.values, y_test.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on new data : holdout data\n",
    "\n",
    "The original competition provides an additional test set, for which you'll never actually see the correct labels. This is called the \"holdout data.\"\n",
    "\n",
    "The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, you can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.\n",
    "\n",
    "Remember that the original goal is to predict the probability of each label. In this exercise you'll do just that by using the .predict_proba() method on your trained model.\n",
    "\n",
    "\n",
    "The point of the holdout data is to provide a fair test for machine learning competitions. If the labels aren't known by anyone but DataCamp, DrivenData, or whoever is hosting the competition, you can be sure that no one submits a mere copy of labels to artificially pump up the performance on their model.\n",
    "\n",
    "Remember that the original goal is to predict the probability of each label:\n",
    "\n",
    "This file containes the target to new data set\n",
    "file = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new data set\n",
    "df_holdout = pd.read_csv('HoldoutData.csv')\n",
    "\n",
    "#prediction on the new dataset\n",
    "predictions = clf.predict_proba(df_holdout[numeric_columns].fillna(-1000))\n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "#Writing out your results to a csv for submission\n",
    "# use your predictions values to create a new DataFrame, prediction_df : \n",
    "\n",
    "\n",
    "# getting the target labels to set for the predictions\n",
    "\n",
    "file = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_2826/datasets/TestSetLabelsSample.csv'\n",
    "holdout_labels = pd.read_csv(file, index_col = 0)\n",
    "\n",
    "\n",
    "# Format predictions in DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(data=predictions, \n",
    "                             columns= pd.get_dummies(holdout_labels[labels]).columns, \n",
    "                             index=holdout_labels.index,)\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model, trained with numeric data only, yields logloss score: 0.19377309958609842\n",
      "\n",
      " Here we got 0.19377 which is much better than 0.2169 that we had with the test data.\n",
      "\n",
      " Even though your basic model scored 0.0 accuracy, it nevertheless performs better than the benchmark score of 2.0455. You've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up your game and incorporate the text data.\n"
     ]
    }
   ],
   "source": [
    "# # Submit the predictions for scoring: score\n",
    "\n",
    "def score_submission(holdout_predictions, holdout_labels):\n",
    "\n",
    "    ## convert the holdout labels to boolean columns\n",
    "    holdout_labels = pd.get_dummies(holdout_labels.apply(lambda x: x.astype('category'), axis=0))\n",
    "    \n",
    "    # make sure that format is correct\n",
    "    assert (holdout_predictions.columns == holdout_labels.columns).all()\n",
    "    assert (holdout_predictions.index == holdout_labels.index).all()\n",
    "\n",
    "    return compute_log_loss(holdout_predictions.values, holdout_labels.values)\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "score = score_submission(prediction_df, holdout_labels)\n",
    "\n",
    "# Print score\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))\n",
    "\n",
    "print('\\n Here we got 0.19377 which is much better than 0.2169 that we had with the test data.')\n",
    "\n",
    "print(\"\\n Even though your basic model scored 0.0 accuracy, it nevertheless performs better than the benchmark score of 2.0455. You've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up your game and incorporate the text data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very brief introduction to NLP : incorporating test data in our model. \n",
    "\n",
    "### from text data to numerical data\n",
    "when we have a Text, documents, speech data we want to process this it to create freatures for our algorithm this is called NLP. The first step in processing this kind of data called **Tokeniation**. \n",
    "\n",
    "**Tokenization** : is the process of splitting a string into segments, this means taking a string and splitting it into a list of strings where we have one string for each word.  \n",
    "\n",
    "*Tokens and token patterns: \n",
    "- Tokenize on whitespace\n",
    "'Petro-Vend Fuel and Fluids' >> ['Petro-Vend','Fuel','and','Fluids'] each token is called 1grams\n",
    "_ Tokenize on whitespace and punctuation\n",
    "'Petro-Vend Fuel and Fluids'>> ['Petro','Vend','Fuel','and','Fluids']\n",
    "\n",
    "\n",
    "*Atfer the tokenization we create a colum for each token and Count the number of times a particular token appears in a row, this is called **Bag of words representation**, this approach discads information about word order, this means that the phrase \"Red, not blue\" is the same as \"blue, not red\". \n",
    "\n",
    "A slightly sophisticated approach than simply counting words is to create what are called **n_grams** :  which is to link multiple words together. \n",
    "single words is 1-gram, two words is 2-gram or bigram and so on.\n",
    "This maintains word order.\n",
    "\n",
    "\n",
    "### Representing text numerically to feed to ML model : \n",
    "\n",
    "the simplest way is to use Bag-of-words becaue it discards information about grammar and word order just by assuming frequency of occurrence of a word is enough information.\n",
    "\n",
    "***Sciket-learn tools for bag-of-words : CountVectorizer()***\n",
    "- Tokenizes all the strings\n",
    "- Builds a 'vocablary' (which is the list of tokens)\n",
    "- Counts the occurrences of each token in the vocabulary\n",
    "\n",
    "####  Creating a bag-of-words in scikit-learn\n",
    "Here we will just practice with one column: The Position_Extra column has additional info not captured in Position_Type, like 'Bus Driver'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            NaN\n",
      "1                            NaN\n",
      "2                        TEACHER\n",
      "3                            NaN\n",
      "4                            NaN\n",
      "5                            NaN\n",
      "6                            NaN\n",
      "7     PROFESSIONAL-INSTRUCTIONAL\n",
      "8                            NaN\n",
      "9                   UNDESIGNATED\n",
      "10                  UNDESIGNATED\n",
      "11    PROFESSIONAL-INSTRUCTIONAL\n",
      "12                  UNDESIGNATED\n",
      "13    PROFESSIONAL-INSTRUCTIONAL\n",
      "14    PROFESSIONAL-INSTRUCTIONAL\n",
      "Name: Position_Extra, dtype: object\n",
      "\n",
      "There are 135 tokens in Position_Extra if we split on white-space only\n",
      "\n",
      "Here are the first 15 tokens:\n",
      "['&', '(no', '-', '1st', '2nd', '3rd', 'a', 'ab', 'additional', 'adm', 'administrative', 'and', 'any', 'art', 'assessment']\n",
      "\n",
      "----------------------------------\n",
      "There are 123 tokens in Position_Extra if we split on non-alpha numeric\n",
      "\n",
      "Here are the first 15 tokens:\n",
      "['1st', '2nd', '3rd', 'a', 'ab', 'additional', 'adm', 'administrative', 'and', 'any', 'art', 'assessment', 'assistant', 'asst', 'athletic']\n",
      "\n",
      "Treating only alpha-numeric characters as tokens gives you a smaller number of more meaningful tokens. You've got bag-of-words in the bag!\n"
     ]
    }
   ],
   "source": [
    "# Creating a bag-of-words in scikit-learn (but we don't count the occurence of each word we counted the sum of tokens)\n",
    "\n",
    "\n",
    "# Import CountVectorizer (counts the number of tokens)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_WHITESPACE = '\\\\S+(?=\\\\s+)'\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# focusing on one feature only: the Position_Extra column, which describes any additional information not captured by the Position_Type label.\n",
    "# column df.Position_Extra before tokenization\n",
    "\n",
    "print(df.Position_Extra[:15])\n",
    "\n",
    "# Fill missing values in df.Position_Extra with empty space (inplace =don't have to assign the result back to df)\n",
    "features = df.Position_Extra.fillna('')\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_whitespace = CountVectorizer(token_pattern = TOKENS_WHITESPACE)\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_whitespace.fit(features)\n",
    "vec_alphanumeric.fit(features)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens \n",
    "\n",
    "msg1 = \"\\nThere are {} tokens in Position_Extra if we split on white-space only\"\n",
    "print(msg1.format(len(vec_whitespace.get_feature_names()))) \n",
    "print('\\nHere are the first 15 tokens:')\n",
    "print(vec_whitespace.get_feature_names()[:15])\n",
    "print('\\n----------------------------------')\n",
    "\n",
    "msg2 = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg2.format(len(vec_alphanumeric.get_feature_names()))) \n",
    "print('\\nHere are the first 15 tokens:')\n",
    "print(vec_alphanumeric.get_feature_names()[:15])\n",
    "\n",
    "print(\"\\nTreating only alpha-numeric characters as tokens gives you a smaller number of more meaningful tokens. You've got bag-of-words in the bag!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining text columns for tokenization \n",
    "In the real world we will use all the text columns to create the set of tokens. Each row will have a bunch of words that represent it.\n",
    "\n",
    "??I guess it does not matter, in this case, which column it comes from.\n",
    "\n",
    "In order to get a bag-of-words representation for all of the text data in our DataFrame, you must first convert the text data in each row of the DataFrame into a single string.\n",
    "\n",
    "In the previous exercise, this wasn't necessary because you only looked at one column of data, so each row was already just a single string. CountVectorizer expects each row to just be a single string, so in order to use all of the text columns, you'll need a method to turn a list of strings into a single string.\n",
    "\n",
    "In this exercise, you'll complete the function definition combine_text_columns(). When completed, this function will convert all training text data in your DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "Note that the function uses NUMERIC_COLUMNS and LABELS to determine which columns to drop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining text columns for tokenization \n",
    "# Define combine_text_columns()\n",
    "\n",
    "def combine_text_columns(data_frame, to_drop= numeric_columns + labels):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector/ phrase after joining all the text in a row \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('',inplace= True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's in a token?\n",
    "\n",
    "Now you will use to convert all training text data in your DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
    "\n",
    "You'll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the first row is now represented by this vector:\n",
      " Supplemental *  Operation and Maintenance of Plant Services    Non-Certificated Salaries And Wages  Care and Upkeep of Building Services    Title I - Disadvantaged Children/Targeted Assistance TITLE I CARRYOVER\n",
      "\n",
      "There are 1384 tokens in the dataset\n",
      "\n",
      "There are 1098 alpha-numeric tokens in the dataset\n",
      "\n",
      "Notice that tokenizing on alpha-numeric tokens reduced the number of tokens, just as in the last exercise. \n"
     ]
    }
   ],
   "source": [
    "#Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the basic token pattern\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern= TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern= TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df.drop('Unnamed: 0', axis=1))\n",
    "print(' the first row is now represented by this vector:\\n', text_vector[0])\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"\\nThere are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric (does this calculate the sum of all tokens or just the sum of unique tokens)\n",
    "print(\"\\nThere are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))\n",
    "\n",
    "print('\\nNotice that tokenizing on alpha-numeric tokens reduced the number of tokens, just as in the last exercise. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supplemental *  Operation and Maintenance of Plant Services    Non-Certificated Salaries And Wages  Care and Upkeep of Building Services    Title I - Disadvantaged Children/Targeted Assistance TITLE I CARRYOVER'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector = combine_text_columns(df.drop('Unnamed: 0', axis=1))\n",
    "text_vector[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B : here we processed the data frame text to incorporate it in our alogrithm but they dind't build the model that takes numeric and precessed text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- mproving your model :\n",
    "\n",
    "Here, you'll improve on your benchmark model using pipelines. Because the budget consists of both text and numeric data, you'll learn to how build pipielines that process multiple types of data. You'll also explore how the flexibility of the pipeline workflow makes testing different approaches efficient, even in complicated problems like this one!\n",
    "\n",
    "## 3.1 Pipelines, feature & text preprocessing :\n",
    "\n",
    "Pipeline is a very flexible way to represent workflow : is a repeatable way to go from raw data to trained model. Pipeline object takes sequential list of steps, and the Output of one step is input to next step, and Each step is a tuple with two elements\n",
    "Name: string\n",
    "Transform: obj implementing .fit() and .transform()\n",
    "\n",
    "either use make_pipeline and passe it the steps\n",
    "or use Pipeline([('name_of the_step', stelp(wich is Transform)])\n",
    "\n",
    "\n",
    "##### we will start by bulding a model that uses only numerical column to predict the lables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampl_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d60eafc06ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m        2.76431471, 3.02431714, 2.59604007, 2.49641543], 'label':['b', 'b', 'a', 'b', 'a', 'b', 'b', 'b', 'a']})\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msampl_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sampl_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Pratice building a pipeline with sample data\n",
    "\n",
    "sample_df = pd.DataFrame({'numeric': [-10.85630603,   9.97345447,   2.82978498, -15.06294714,\n",
    "        -5.78600252,  16.51436537, -24.26679243,  -4.28912629,\n",
    "        12.65936259], 'text':['', 'foo', 'foo bar', '', 'foo bar', '', 'foo bar', 'foo', ''], 'with_missing':[4.43324013, 4.31022893, 2.46982849, 2.85298126, 1.8264749 ,\n",
    "       2.76431471, 3.02431714, 2.59604007, 2.49641543], 'label':['b', 'b', 'a', 'b', 'a', 'b', 'b', 'b', 'a']})\n",
    "\n",
    "sampl_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - numeric, no nans:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import other necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Split and select numeric data only, no nans \n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all numeric, incl nans:  1.0\n"
     ]
    }
   ],
   "source": [
    "#  incorporate numeric data with missing values by adding a preprocessing step\n",
    "\n",
    "# improve pipeline by using the Imputer() to fill in missing values in your sample data(replaces NaNs with the mean value of the column).\n",
    "\n",
    "# Import the Imputer object\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create training and test sets using only numeric data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "\n",
    "# Insantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bulding a model that uses only text to predict the lables : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on sample data 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# import CountVectoriser and pipeline \n",
    "\n",
    "# Create training and test sets using only text data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "# Insantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('vec', CountVectorizer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print('accuracy on sample data',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text features and feature unions\n",
    "\n",
    "#### Building a model based on numerical data and text data by processing multiple data type: \n",
    "\n",
    "The problem is that we can not build a pipeline that has CountVectoriser Step, Impter step and classifier, because we feed the pipeline the data and all the data goes by those steps but the we can not apply CountVectoriser numerical data or imputer on text data. \n",
    "In oder to build a pipeline we need to separetly operate on the text columns and the numerical columns. there are 2 tools : **FunctionTransformer** and **FeatureUnion** that will help us work with both text and numerical data. \n",
    "\n",
    "##### 1- Function Transformer : \n",
    "Turns a Python function into an object that a scikit-learn pipeline can understand\n",
    "\n",
    "we will need to write two functions for pipeline preprocessing : \n",
    "_ the first takes entire DataFrame, return numeric columns\n",
    "_ the second takes entire DataFrame, return text columns\n",
    "\n",
    "using those function transformers we can build a separate pipline for our numeric data and for out text data\n",
    "\n",
    "##### 2- feature Union : \n",
    "joins the numerical array and text array after being processed in one signle array that will be the input to the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data\n",
      "0           \n",
      "1        foo\n",
      "2    foo bar\n",
      "3           \n",
      "4    foo bar\n",
      "Name: text, dtype: object\n",
      "----------------------\n",
      "\n",
      "Numeric Data\n",
      "     numeric  with_missing\n",
      "0 -10.856306      4.433240\n",
      "1   9.973454      4.310229\n",
      "2   2.829785      2.469828\n",
      "3 -15.062947      2.852981\n",
      "4  -5.786003      1.826475\n",
      "\n",
      "Accuracy on sample data - all data:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "# Split using ALL data in sample_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric', 'with_missing', 'text']],\n",
    "    pd.get_dummies(sample_df['label']), \n",
    "    random_state=22)\n",
    "\n",
    "\n",
    "# Obtain the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "# Obtain the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "\n",
    "####################\n",
    "#testing \n",
    "# Fit and transform the text data: just_text_data\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# Fit and transform the numeric data: just_numeric_data\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "# Print head to check results\n",
    "print('Text Data')\n",
    "print(just_text_data.head())\n",
    "print('----------------------')\n",
    "print('\\nNumeric Data')\n",
    "print(just_numeric_data.head())\n",
    "###########################\n",
    "\n",
    "#this supposed to work but it doesn't, I used the same idea but i guess it needs .fit_transform\n",
    "# # process text data:  \n",
    "# text_pipeline = Pipeline([('get_text', get_text_data),('vec', CountVectorizer())])\n",
    "# # process numeric data: \n",
    "# numeric_pipeline = Pipeline([('get_numeric', get_numeric_data),('imputer', Imputer())])\n",
    "\n",
    "# # FeatureUnion joining processed numeric and text data :\n",
    "# union = FeatureUnion([('numeric', numeric_pipeline), ('text', text_pipeline)]\n",
    "                                  \n",
    "# #Instantiate nested pipeline: pl\n",
    "# big_pipeline = Pipeline([('union', union), ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "#############\n",
    "\n",
    "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit pl to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a classification model: \n",
    "\n",
    "##### Using FunctionTransformer on the main dataset and than building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = df.set_index(df['Unnamed: 0'],)\n",
    "L = j.drop('Unnamed: 0', axis=1)\n",
    "# L.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Object_Description', 'Text_2', 'SubFund_Description', 'Job_Title_Description', 'Text_3', 'Text_4', 'Sub_Object_Description', 'Location_Description', 'FTE', 'Function_Description', 'Facility_or_Department', 'Position_Extra', 'Total', 'Program_Description', 'Fund_Description', 'Text_1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Size less than number of columns * min_count, returning 520 items instead of 312.0.\n"
     ]
    }
   ],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels Y\n",
    "dummy_labels = pd.get_dummies(L[labels])\n",
    "\n",
    "# Get the columns that are features in the original df X\n",
    "NON_LABELS = [column for column in L.columns if column not in labels]\n",
    "print(NON_LABELS)\n",
    "# Split into training and test sets\n",
    "XX_train, XX_test, yy_train, yy_test = multilabel_train_test_split(L[NON_LABELS], dummy_labels, 0.2, seed=123)\n",
    "\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(\n",
    "    combine_text_columns,\n",
    "    validate = False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(\n",
    "    lambda x: x[numeric_columns], validate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.20384615384615384\n"
     ]
    }
   ],
   "source": [
    "## ADD model to pipeline : \n",
    "# Complete the pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(XX_train, yy_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(XX_test, yy_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try a different class of model\n",
    "Look how easy it is to change the model\n",
    "Here we will swap out the logistic-regression model and replace it with a random forest classifier, which uses the statistics of an ensemble of decision trees to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.28076923076923077\n"
     ]
    }
   ],
   "source": [
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit to the main training data \n",
    "pl.fit(XX_train, yy_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(XX_test, yy_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy) ## THERE IS SOMETHING WRONG IT SHOULD BE 0.904877896446"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you adjust the model or parameters to improve accuracy?\n",
    "Here we just do a bit of parameter tuning to improve accuracy\n",
    "It would be nicer if I could dynamically set the model in a variable and then just rerun the same code.\n",
    "But I will just follow along with the class code for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.3211538461538462\n",
      "That seems accurate given the nature of the data noe lets see what the log loss of the tweeked model will be.\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add model step to pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators = 15))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(XX_train, yy_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(XX_test, yy_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy) ## THERE IS SOMETHING WRONG\n",
    "\n",
    "print('That seems accurate given the nature of the data noe lets see what the log loss of the tweeked model will be.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Learning from the experts: \n",
    "\n",
    "\n",
    "The model that won used those tricks : \n",
    "- NLP: Range of n-grams, punctuation tokenization\n",
    "- Stats: Interaction terms\n",
    "- Computation: Hashing trick\n",
    "\n",
    "What class of model was used? Logistic Regression? Yep\n",
    "\n",
    "Sometime a simple model with smarlty chosen feature engineering is all thats needed.\n",
    "And its usually better to start with simple models that can also offer explaination.\n",
    "\n",
    "## 4.1 processing tricks to get better score :  text processing, statistical methods, computational efficency \n",
    "\n",
    "by knowing which tricks to use and which tools to combine, we can build extremely effective models.\n",
    "\n",
    "1/ text processing : \n",
    "\n",
    "NLP tricks used for text data : \n",
    "- **Tokenization trick** : Tokenize on punctuation to avoid hyphens, underscores, etc\n",
    "_ **n_grams trick** : Include unigrams and bi-grams in the model to cature important information involving multiple tokens - e.g. 'middle school'.      \n",
    "\n",
    "\n",
    "\n",
    "*we can change the way we preprocess text data N-grams and tokenization by simply changing COuntVectorizer function on the pipeline: \n",
    "\n",
    "vec = COuntVectorizer(token_pattern = alphanumeric_tokenization , ngram_range = (1, 2))   \n",
    "\n",
    "\n",
    "\n",
    "N.B : the way you tokenize text affects the n-gram statistics used in your model.\n",
    "We will use alpha-numeric sequences, and only alpha-numeric sequences, as tokens. Alpha-numeric tokens contain only letters a-z and numbers 0-9 (no other characters). In other words, you'll tokenize on punctuation to generate n-gram statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00a', '12', '1st', '2nd', '4th', '5th', '70h', '8', 'a', 'ab', 'acad', 'academ', 'academic', 'academy', 'accelerated', 'access', 'accountability', 'accounting', 'accounts', 'achieve', 'acq', 'act', 'activi', 'activiti', 'activities', 'activity', 'additional', 'addl', 'aditional', 'admin']\n"
     ]
    }
   ],
   "source": [
    "# **Tokenization trick** : how the text features will be processed using alphanumeric pattern \n",
    "#Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(XX_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer: text_features\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(text_features.get_feature_names()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00a', '00a office', '00a supplies', '12', '12 general', '12 instruction', '12 reserved', '1st', '1st grade', '2nd', '2nd grade', '4th', '4th grade', '5th', '5th grade', '70h', '8', '8 education', '8 general', '8 instruction', '8 primary', 'a', 'a arra', 'a improving', 'a instr', 'a legacy', 'a lep', 'a professional', 'a schoolwide', 'a super']\n",
      "\n",
      " Note : we can see that text processing based on alpha numeric token pattern and ngram_range gives a diffrent result than tokenizing only with alphanumeric pattern\n"
     ]
    }
   ],
   "source": [
    "# *n_grams trick** compute multiple n-gram features to be used in the model.\n",
    "# N-gram range in scikit-learn\n",
    "\n",
    "#In order to look for ngram relationships at multiple scales, you will use the ngram_range(1,2)\n",
    "\n",
    "#Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(XX_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer: text_features  with ngram_range\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(text_features.get_feature_names()[:30])\n",
    "\n",
    "print('\\n Note : we can see that text processing based on alpha numeric token pattern and ngram_range gives a diffrent result than tokenizing only with alphanumeric pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Building the wining model : with text processing tricks \n",
    "\n",
    "Special functions: You'll notice a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the dim_red step following the vectorizer step , and the scale step preceeding the clf (classification) step.\n",
    "\n",
    "These have been added in order to account for the fact that you're using a reduced-size sample of the full dataset in this course. To make sure the models perform as the expert competition winner intended, we have to apply a dimensionality reduction technique, which is what the dim_red step does, and we have to scale the features to lie between -1 and 1, which is what the scale step does.\n",
    "\n",
    "The dim_red step uses a scikit-learn function called SelectKBest(), applying something called the chi-squared test to select the K \"best\" features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.19230769230769232\n",
      "Well its much better than the previous logisitic regression model But not nearly as good as the Random forest model\n"
     ]
    }
   ],
   "source": [
    "# wining model pipeline : \n",
    "\n",
    "\n",
    "# Import pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import classifiers\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "#from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[numeric_columns], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "#So how much better is this?\n",
    "# Fit to the training data\n",
    "pl.fit(XX_train, yy_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(XX_test, yy_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)\n",
    "\n",
    "print('Well its much better than the previous logisitic regression model But not nearly as good as the Random forest model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 add some additional tricks to make the pipeline even better : a stats trick \n",
    "\n",
    "we use bigrams(n_gram =2) to count for when words apprear in a certain order, but what if the words are not next each other. Solutin :  The statistical tool that the winner have used is called **Interaction terms** : it let us mathematically describe when tokens appear together.\n",
    "\n",
    "Example : \n",
    "- English teacher for 2nd grade\n",
    "- 2nd grade - budget for English teacher\n",
    "\n",
    "math explanation for interaction terms : Î²1x1+Î²2x2+Î²3(x1Ã—x2)\n",
    "\n",
    "*Î²1x1*\n",
    "x1 represent token and it equals if present or 0 if it's not\n",
    "Î²1 is a coefficient that represents how important the tokens is \n",
    "\n",
    "*Same for Î²2x2*\n",
    "\n",
    "*Î²3(x1Ã—x2)* is the interaction term btwn words x1 and x2 if they appear together\n",
    "Î²3 measurs the importance of both words appearing together \n",
    "\n",
    "\n",
    "#### Adding interaction features with scikit-learn \n",
    "\n",
    "interaction features in sklearn are called polynomial features \n",
    "\n",
    "Sparse interaction features\n",
    "The number of interaction terms grows exponentially\n",
    "Our vectorizer saves memory by using a sparse matrix\n",
    "Polynomial Features does not support sparse matrices\n",
    "\n",
    "\n",
    "#### Implement interaction modeling in scikit-learn pipeline\n",
    "It's time to add interaction features to your model. The PolynomialFeatures object in scikit-learn does just that, but here you're going to use a custom interaction object, SparseInteractions. Interaction terms are a statistical tool that lets your model express what happens if two features appear together in the same row.\n",
    "\n",
    "**SparseInteractions** does the same thing as PolynomialFeatures, but it uses sparse matrices to do so. You can get the code for SparseInteractions at this GitHub Gist.https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py\n",
    "\n",
    "PolynomialFeatures and SparseInteractions both take the argument degree, which tells them what polynomial degree of interactions to compute.\n",
    "\n",
    "You're going to consider interaction terms of degree=2 in your pipeline. You will insert these steps after the preprocessing steps you've built out so far, but before the classifier steps.\n",
    "\n",
    "Pipelines with interaction terms take a while to train (since you're making n features into n-squared features!), so as long as you set it up right, we'll do the heavy lifting and tell you what your score is!We have provided SparseInteractions to work for this problem\n",
    "the function code is provided \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2,interaction_only = True, include_bias = False)\n",
    "\n",
    "# interaction.fit_transform(x)\n",
    "# x should be text data\n",
    "\n",
    "\n",
    "# Notes:\n",
    "# degree means compare 2 words if they appear together\n",
    "# the interaction_only term means don't multiple a term by itself\n",
    "# the include bias term allows model to have non-zero y value when x value is zero.\n",
    "# Its like the intercept value for a regression line I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "##**SparseInteractions** \n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.19230769230769232\n",
      "\n",
      " it is not a good idea to run this model on the full dataset without using the last technique **SparseInteractions**  to increase computation efficiency, This models takes an extremely long time. The number of features grows exponentially when you add interaction terms. We can limit the number of features by using a hash table This will be covered next.\n"
     ]
    }
   ],
   "source": [
    "##  Implement interaction modeling in scikit-learn pipeline \n",
    "\n",
    "l = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "#So how much better is this?\n",
    "# Fit to the training data\n",
    "pl.fit(XX_train, yy_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(XX_test, yy_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)\n",
    "\n",
    "print(\"\\n it is not a good idea to run this model on the full dataset without using the last technique **SparseInteractions**  to increase computation efficiency, This models takes an extremely long time. The number of features grows exponentially when you add interaction terms. We can limit the number of features by using a hash table This will be covered next.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 a computational trick and the winning model : Hashing trick \n",
    "\n",
    "Hashing trick : \n",
    "- Adding new features ngram_range =(1,2,3) may cause enormous increase in array size\n",
    "- Hashing is a way of increasing memory efficiency\n",
    "- Hash function limits possible outputs, fixing array size\n",
    "- multiple tokens may be added to the same hash, but this has been shown (in research papers) to not have much of a negative impact on the models ability to predict. Thats, um, pretty neat.\n",
    "\n",
    "When to use the hashing trick: \n",
    "- Want to make array of features as small as possible\n",
    "- - Dimensionality reduction\n",
    "- Particularly useful on large datasets e.g., lots of text data!\n",
    "\n",
    "\n",
    "a hash function takes an input, in your case a token word, and outputs a hash value number. so instead of using CountVectoriser that retuns a bag of word we use HashingVectorizer that return intergers. \n",
    "\n",
    "Some computation problems are memory-bound and not easily parallelizable, and hashing enforces a fixed length computation instead of using a mutable datatype (like a dictionary). By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets.(Enforcing a fixed length can speed up calculations drastically, especially on large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vec = HashingVectorizer(\n",
    "    norm = None,\n",
    "    non_negative = True,\n",
    "    token_pattern = TOKENS_ALPHANUMERIC,\n",
    "    ngram_range = (1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 780, 'fluids': 354, 'fuel': 895, 'petro': 354, 'vend': 785}\n"
     ]
    }
   ],
   "source": [
    "#Ex of how hasing works : \n",
    "\n",
    "hash_dict = {'and': 780, 'fluids': 354, 'fuel': 895, \n",
    "             'petro': 354, 'vend': 785}\n",
    "print(hash_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0 -0.162221\n",
      "1  0.162221\n",
      "2 -0.486664\n",
      "3 -0.324443\n",
      "4  0.162221\n"
     ]
    }
   ],
   "source": [
    "# how hashingvectoriser will work on our data \n",
    "\n",
    "# Import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import  HashingVectorizer\n",
    "\n",
    "# Get text data: text_data\n",
    "text_data = combine_text_columns(XX_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(token_pattern = TOKENS_ALPHANUMERIC )\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_df.head())\n",
    "\n",
    "\n",
    "print('HashingVectorizer retuns integers instead of bag of words and as you can see, some text is hashed to the same value ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 : building the model's pipeline that contains all the tricks \n",
    "\n",
    "The winning model used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data\n",
    "\n",
    "You've constructed a robust, powerful pipeline capable of processing training and testing data. Now that you understand the data and know all of the tools you need, you can essentially solve the whole problem in a relatively small number of lines of code. Wow!\n",
    "\n",
    "All you need to do is add the HashingVectorizer step to the pipeline to replace the CountVectorizer step.\n",
    "\n",
    "The parameters non_negative=True, norm=None, and binary=False make the HashingVectorizer perform similarly to the default settings on the CountVectorizer so you can just replace one with the other.\n",
    "\n",
    "** the wining model code ** : https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb\n",
    "\n",
    "If you want to use this model locally, this Jupyter notebook contains all the code you've worked so hard on. You can now take that code and build on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.3173076923076923\n"
     ]
    }
   ],
   "source": [
    "# building the model's pipeline that contains all the tricks\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     non_negative=True, norm=None, binary=False,\n",
    "                                                     ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(XX_train, yy_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(XX_test, yy_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
